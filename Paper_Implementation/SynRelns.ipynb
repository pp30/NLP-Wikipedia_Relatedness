{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SynRelns.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"46-Fq0yQMr3V","colab_type":"code","outputId":"5f2ed36a-3e4a-491b-b256-deea2d52b10d","executionInfo":{"status":"ok","timestamp":1586200685572,"user_tz":-330,"elapsed":58658,"user":{"displayName":"PAWAN PRASAD K me16b179","photoUrl":"","userId":"14049130637097040281"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n","/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9QFm1ZPgNQDF","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","import math\n","from gensim.models import KeyedVectors\n","from tqdm.notebook import tqdm\n","os.chdir('/gdrive/My Drive/NLP_Project_Programs/Paper_Implementation')\n","#os.listdir()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B5L--bWMC0En","colab_type":"code","colab":{}},"source":["path = 'Evaluation_Benchmarks/Syntactic_Relns/syntactic_alone.txt'  # path to Syntactic Relations evaluation dataset file\n","obj = open(path,'r')\n","syn_words = []\n","for i,line in enumerate(obj):\n","  syn_words.append(line.strip().lower().split())\n","# REDUCE THE VECTOR SPACE OF SKIPGRAM BY TAKING ONLY THOSE WORDS IN SYN-REL DATASET AND DISCARD THE REST.\n","# THIS WILL SPEED UP THE COSINE SIMILARITY CALCULATION (HOPEFULLY).\n","\n","# Obtain all the unique words\n","unique_words = []\n","for line in syn_words:\n","    for word in line:\n","        unique_words.append(word)\n","unique_words = set(unique_words)\n","\n","def reduce_vectors(words, vectors):\n","    new_words = []\n","    new_vectors = []\n","    \n","    for word, vector in zip(words, vectors):\n","        if word in unique_words:\n","            new_words.append(word)\n","            new_vectors.append(vector)\n","\n","    unique_reduced_words = set(new_words)\n","    missing_words = unique_words - unique_words.intersection(unique_reduced_words)\n","    for word, vector in tqdm(zip(words, vectors)):\n","        word = word.lower()\n","        if word in missing_words:\n","            new_words.append(word)\n","            new_vectors.append(vector)\n","    return new_words, new_vectors\n","\n","def SynRel_model(words, vectors, vector_dimensions):\n","  \"\"\"\n","  Computes accuracy for the task of selecting the word closest analogous word d such that a:b::c:d using cosine similarity\n","\n","  Parameters\n","  ----------\n","  words : list\n","    A list containing only words (dtype = string) from given word vector file\n","\n","  vectors : list\n","    A list of lists with each sub-list as vector (dtype = float) for corresponding word from given word vector file  \n","\n","  vector_dimensions : int \n","    The length of the vectors present in given word-vector file\n","\n","  Returns\n","  -------\n","  arg1 : float\n","    accuracy score for the evaluation task\n","  \"\"\"\n","  count = 0\n","  missed_count=0\n","  accuracy = 0\n","  vec_mat = np.array(vectors)\n","  #mean_vector = np.mean(vec_mat, axis=0)\n","\n","  for index,wlist in tqdm(enumerate(syn_words)):\n","    #if index%1000==0: print('-'*15, index,'/ 10,000, \\t Score :', count*100/(index+1), count*100/(index-missed_count+1))\n","    w4 = wlist[3]\n","    w_mat = np.zeros((3,vector_dimensions))\n","    # for i in range(3):\n","    #     w_mat[3] == mean_vector\n","    query_indices = []\n","    query_index = 0\n","    for word,vec in zip(words,vectors):\n","      if wlist[0] == word:\n","        w_mat[0,:] = vec \n","        query_indices.append(query_index)\n","      if wlist[1] == word:\n","        w_mat[1,:] = vec\n","        query_indices.append(query_index)\n","      if wlist[2] == word: \n","        w_mat[2,:] = vec\n","        query_indices.append(query_index)\n","      query_index+=1\n","    # print(query_indices)\n","    # break\n","    temp_bool = w_mat == 0\n","    if np.sum(temp_bool[0,:])==300:\n","      missed_count+=1\n","    if np.sum(temp_bool[1,:])==300:\n","      missed_count+=1\n","    if np.sum(temp_bool[2,:])==300:\n","      missed_count+=1\n","\n","    q_vec = w_mat[1,:]-w_mat[0,:]+w_mat[2,:]\n","    q_vec = q_vec.reshape(1,vector_dimensions)\n","\n","    '''--------------------- FOR DEBUGGING ------------------------'''\n","    # top_2_ind = np.argsort(cosine_similarity(q_vec, vec_mat))[0,-2:]\n","    #q_list.append(words[ind])\n","    # if words[top_2_ind[1]] == wlist[2]:\n","    #   if words[top_2_ind[0]] == w4:\n","    #     count += 1\n","    # elif words[top_2_ind[1]] == w4:\n","    #   count += 1\n","    # if (words[top_2_ind[0]] == w4) or (words[top_2_ind[1]] == w4):\n","    #     count += 1\n","\n","    vec_without_query = np.delete(vec_mat, query_indices,0)\n","    words_without_query = np.delete(words, query_indices,0)\n","    # print('new vec shape :', np.shape(vec_without_query), 'Old shape :', np.shape(vec_mat))\n","    # break\n","    top_2_ind = np.argsort(cosine_similarity(q_vec, vec_without_query))[0,-2:]\n","    # if (words[top_ind[0]] == w4) or (words[top_ind[1]] == w4):\n","    if words_without_query[top_2_ind[1]] == wlist[2]:\n","      if words_without_query[top_2_ind[0]] == w4:\n","        count += 1\n","    elif words_without_query[top_2_ind[1]] == w4:\n","      count += 1\n","\n","  print('->'*15, 'Total Missed :', missed_count)\n","  accuracy = count/len(syn_words)\n","\n","  return accuracy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZUJNmRxNHAON","colab_type":"code","colab":{}},"source":["#Global Context word vectors file\n","WordVec_path_a = 'Vector Representation of Words/3/3_word_vectors.txt'\n","#Skip Gram (SG) word vectors file\n","WordVec_path_b = 'Vector Representation of Words/2/GoogleNews-vectors-negative300.bin'\n","#Glove word vector file\n","WordVec_path_c = 'Vector Representation of Words/1/glove.6B.300d.txt'\n","\n","#Global Context retrofitted word vectors file\n","outfile_path_a1 = 'Vector Representation of Words/3/RETRO_FrameNet_3_word_vectors.txt' \n","outfile_path_a2 = 'Vector Representation of Words/3/RETRO_ppdb-xl_3_word_vectors.txt' \n","outfile_path_a3 = 'Vector Representation of Words/3/RETRO_wordnet-synonyms_3_word_vectors.txt' \n","outfile_path_a4 = 'Vector Representation of Words/3/RETRO_wordnet-synonyms+_3_word_vectors.txt' \n","\n","#Glove retrofitted word vector files\n","outfile_path_c1 = 'Vector Representation of Words/1/RETRO_FrameNet_glove.6B.300d.txt'\n","outfile_path_c2 = 'Vector Representation of Words/1/RETRO_ppdb-xl_glove.6B.300d.txt'\n","outfile_path_c3 = 'Vector Representation of Words/1/RETRO_wordnet-synonyms_glove.6B.300d.txt'\n","outfile_path_c4 = 'Vector Representation of Words/1/RETRO_wordnet-synonyms+_glove.6B.300d.txt'\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OyZ0cO_WT1y2","colab_type":"code","colab":{}},"source":["'''Evaluating on the Glove and GC word vectors file (including retrofitted Glove and GC word vectors)'''\n","\n","''' Read all the word vectors and normalize them '''\n","def read_word_vecs(filename):\n","  # Use this for Glove and Global Context Vectors\n","  words = []\n","  vectors = []\n","  fileObject = open(filename, 'r')\n","  for index,line in enumerate(fileObject):\n","    # if index%50000==0:print('->', index,'/ 3000000')\n","    line = line.strip().lower()\n","    words.append(line.split()[0])\n","    vec = np.zeros(len(line.split())-1, dtype=float)\n","    for ind, vecVal in enumerate(line.split()[1:]):\n","      vec[ind] = float(vecVal)\n","    ''' normalize weight vector '''\n","    # vec /= math.sqrt((vec**2).sum() + 1e-6)\n","    vectors.append(vec)\n","    \n","  return words, vectors\n","\n","''' change 2nd argument inside tuple to 50 or 300 accordingly for Global context (50) and Glove word vector files respectively '''   \n","filenames = [(outfile_path_a4,50), (outfile_path_c4,300)]\n","\n","for filename in filenames:\n","  words,vectors = read_word_vecs(filename[0])\n","  words, vectors = reduce_vectors(words,vectors) \n","  print('word vec generated for :', filename)\n","  # accuracy_score= SynRel_model(words,vectors,vector_dimensions=filename[1])\n","  accuracy_score = SynRel_model(words,vectors,vector_dimensions=filename[1])\n","  print('Accuracy :',accuracy_score)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1TYNUVgCUmNb","colab_type":"code","colab":{}},"source":["'''Evaluating on the SG (skip gram) word vectors file (does not include retrofitted SG word vectors)'''\n","\n","filename = WordVec_path_b \n","def read_word_vectors_skipgram():\n","  # Use this for Google skipgram word2vec model\n","  word_vec_bin = KeyedVectors.load_word2vec_format(filename, binary=True)\n","  return word_vec_bin\n","\n","## function to convert word_vec to (words, vectors)\n","def my_function(word_vec):\n","  words = []\n","  vectors = []\n","  vocabulary = word_vec.vocab\n","  for word in tqdm(vocabulary):\n","      words.append(word)  \n","      vectors.append(word_vec[word])\n","  \n","  return words, vectors    \n","\n","word_vec = read_word_vectors_skipgram()\n","words, vectors = my_function(word_vec)\n","words, vectors = reduce_vectors(words,vectors)\n","print('word vec generated for :', filename)\n","accuracy_score= SynRel_model(words,vectors,vector_dimensions=filename[1])\n","print('Accuracy :',accuracy_score)\n"],"execution_count":0,"outputs":[]}]}